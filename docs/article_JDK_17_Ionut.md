## JIT Compilers

### DeadMethodCallStoreBenchmark

The benchmark assesses how the compiler could remove code (i.e., a dead method call store) that does not affect the program results.
Dead-Code Elimination (DCE) is a compiler optimization to remove code which does not affect the program results.

Source code: <<link to GitHub benchmark>>

<<IMG: DeadMethodCallStoreBenchmark.svg>>

#### Conclusions:

Looking at the assemnly generated by the `method_call_dse` we can summarize:

- OpenJDK HotSpot C2 JIT does inlining of all the methods, nevertheless, it cannot completly remove the loops pertaining to the dead store method calls.

For example, the below snapshot appears 4 times in the caller method, which correspond to the number of dead loops after inlining:
```
  0x7fffe0ef1793:   add    $0x20,%r10d        ;*iinc
  0x7fffe0ef17a0:   cmp    $0x3e2,%r10d       ;compare against 994
  0x7fffe0ef17a7:   jl     0x7fffe0ef1793     ;*if_icmpge
```
Then, the main loop (that one that matters for the result) it is unrolled by a factor of 16.

- GraalVM CE JIT has a similar behaviour as OpenJDK HotSpot C2 JIT, nevertheless, there is one more important difference: it does not do any loop unrolling, this is why it is the slowest
```
  0x7fffdef45310:   cmp    $0x3e8,%r10d       ;compare against 1000 (i.e., the loop limit)
  0x7fffdef45317:   jge    0x7fffdef4534c     ;*if_icmpge
  ...
  0x7fffdef4532d:   lea    0x2(%r10),%r10d    ;*iinc by loop stride
  ...
  0x7fffdef4534a:   jmp    0x7fffdef45310
```

- GraalVM EE JIT complently removes the dead code and the main loop is unrolled by a factor of 8.

Looking at the assemnly generated by the `method_call_baseline()` we can summarize:

- GraalVM EE JIT triggers the loop unrolling for the main loop, then the remaining elements are handled by two post loops (i.e., standard loop optimizations)
- OpenJDK HotSpot C2 JIT handles it in a similar manner as GraalVM EE JIT (in terms of loop optimizations), nevertheless the generated assembly instructions are less optimal
- GraalVM CE JIT does not trigger any loop unrolling

### EnumValueLookupBenchmark

This benchmark iterates through the enum values list and returns the enum constant corresponding to an (e.g., a String) value. It emphasizes the cost of calling the values() method on an Enum versus caching the values and using the cached version.
Note: the enum's method values() returns a new copy of an array representing its values every time it is called.

This pattern is often seen in real business applications where, for example, the microservices RESTful APIs defined in OpenAPI/Swagger use enums.
The input request parameters are deserialized and wrapped to enum values.

Source code: <<link to GitHub benchmark>>

<<IMG: EnumValueLookupBenchmark.svg>>

#### Conclusions:

The `enum_values` benchmark is impacted by the object allocations triggered by the `enum::values()` call that is an invoke virtual to `clone()` method.

Caching these values and using the cached structure instead of calling `enum::values()` reduces the number of allocations.

### IfConditionalBranchBenchmark

Tests the conditional branch optimizations within a loop using:
- a predictable branch pattern
- an unpredictable branch pattern
- no branch at all

Source code: <<link to GitHub benchmark>>

<<IMG: IfConditionalBranchBenchmark.svg>>

#### Conclusions:

OpenJDK HotSpot C2 JIT

- for the simplest case (e.g., `no_if_branch`) the C2 JIT unrolls the loop by a factor of 8 and sums the elements array
```
  0x7fa610ef2140:   add    0x10(%r9,%rcx,4),%eax
  0x7fa610ef2145:   add    0x14(%r9,%rcx,4),%eax
  0x7fa610ef214a:   add    0x18(%r9,%rcx,4),%eax
  0x7fa610ef214f:   add    0x1c(%r9,%rcx,4),%eax
  0x7fa610ef2154:   add    0x20(%r9,%rcx,4),%eax
  0x7fa610ef2159:   add    0x24(%r9,%rcx,4),%eax
  0x7fa610ef215e:   add    0x28(%r9,%rcx,4),%eax
  0x7fa610ef2163:   add    0x2c(%r9,%rcx,4),%eax  ;*iadd 
  0x7fa610ef2168:   add    $0x8,%ecx              ;*iinc 
  0x7fa610ef216b:   cmp    %r11d,%ecx
  0x7fa610ef216e:   jl     0x7fa610ef2140         ;*if_icmpge
```

- for the `predictable_if_branch` and `unpredictable_if_branch` benchmarks, the C2 JIT unrolls the loop by a factor of 8 and sums the elements array, nevertheless, to handle the branches, conditional instructions are emitted in the assembly code

This is an example using _cmovle_ instruction from the `unpredictable_if_branch` that adds one element array to the sum

```
  0x7f5ebcef3471:   lea    (%r12,%rbp,8),%r14        ;*getfield array
  ...
  0x7f5ebcef34e0:   mov    0x10(%r14,%rbp,4),%r10d
  ...
  0x7f5ebcef3508:   mov    %eax,%edx
  0x7f5ebcef350a:   add    %r10d,%edx
  0x7f5ebcef350d:   cmp    $0x800,%r10d              ;branch conditional value 2048
  0x7f5ebcef3514:   cmovle %edx,%eax
```

GraalVM EE JIT
- in all the cases GraalVM EE JIT does to sum of elements array using vectorized instructions. This is the explanation why it is the fastest

This is an example using vectorized instructions from the `unpredictable_if_branch`:

```
  0x7f860699030b:   mov    $0x0,%r8
  0x7f8606990312:   data16 nopw 0x0(%rax,%rax,1)
  0x7f860699031c:   data16 data16 xchg %ax,%ax
  0x7f8606990320:   vmovdqu (%rax,%r8,4),%ymm2
  0x7f8606990326:   vpaddd %ymm2,%ymm0,%ymm3
  0x7f860699032a:   vpcmpgtd %ymm2,%ymm1,%ymm2
  0x7f860699032e:   vpblendvb %ymm2,%ymm3,%ymm0,%ymm0
  0x7f8606990334:   lea    0x8(%r8),%r8
  0x7f8606990338:   cmp    %r11,%r8                    ;compare against array length
  0x7f860699033b:   jle    0x7f8606990320
```

GraalVM CE JIT
- in case of `no_if_branch` it unrolls the loop by a factor of 16, but the emitted instruction code is less optimal than in the case of OpenJdk HotSpot C2 JIT
- in case of `predictable_if_branch` and `unpredictable_if_branch` it does not unroll the loop, this is why it is also the slowest

### LockCoarseningBenchmark

Test how the compiler can effectively coarsen/merge several adjacent synchronized blocks into one synchronized block, thus reducing the locking overhead. This optimization can be applied if the same lock object is used by all methods. Compilers can help to coarsen/merge the locks, but that is not guaranteed!

OpenJDK HotSpot VM specifics:
- prior JDK 15: with biased locking enabled, compare-and-swap atomic operation are basically no-ops when acquiring a monitor, in case of uncontended locking. It assumes that a monitor remains owned by a given thread until a different thread tries to acquire it
- starting JDK 15: without biased locking (or some improved version of non-biased locking), certain synchronized scenarios might become much slower (i.e., since synchronized calls comes now with atomic compare-and-swap on lock)

*Note:* ZGC and Shenandoah GC have biased locking disabled to prevent safepoint operations (e.g., biased locking revocation), hence avoiding stop-the-world pauses.

Source code: <<link to GitHub benchmark>>

<<IMG: LockCoarseningBenchmark_withBiasedLocking.svg>>

#### Conclusions:

- Overall GraalVM EE JIT does a better job then GraalVM CE JIT and OpenJDK HotSpot C2 JIT

What is of a particular interest is the `nested_synchronized` benchmark that is almost 3 orders of magnitude slower than GraalVM CE/EE.

```
  public int nested_synchronized() {
  int result = defaultValue << 1;

      synchronized (this) {
        result += incrementValue;
        synchronized (this) {
          result += incrementValue;
          ...
        }
      }

      return result;
}
```

The reason for that is the fact the compiler fails to reduce the deoptimization rate (i.e., hits a recompilation limit) and the method is abandoned to the Template Interpreter, as described by the compilation logs:

```
<make_not_compilable osr='0' level='3' reason='MethodCompilable_not_at_tier' method='LockCoarseningBenchmark nested_synchronized ()I'/>
<make_not_compilable osr='0' level='4' reason='MethodCompilable_not_at_tier' method='LockCoarseningBenchmark nested_synchronized ()I'/>
```

The hottest regions in the report emphasizes this:

```
  ....[Hottest Regions]...............................................
  12.94%        interpreter  monitorenter  194 monitorenter  
  10.15%        interpreter  fast_iaccess_0  221 fast_iaccess_0  
  9.18%         interpreter  monitorexit  195 monitorexit  
  9.00%         interpreter  goto  167 goto  
  7.74%         interpreter  dup  89 dup  
  7.01%         interpreter  istore_1  60 istore_1  
  6.60%         interpreter  fast_aload_0  220 fast_aload_0  
  5.98%         interpreter  iadd  96 iadd  
  5.90%         interpreter  aload  25 aload  
  5.38%         interpreter  astore  58 astore  
  3.45%         interpreter  iload_1  27 iload_1  
  2.25%         interpreter  monitorexit  195 monitorexit  
  1.13%         interpreter  method entry point (kind = zerolocals)
```

By contrast, GraalVM EE JIT can coarse all the nested locks in one block and perform all the operations inside. Nevertheless, since biased locking is enabled and there is no contention this is a no-op when acquiring a monitor, hence no `lock cmpxchg` instruction is emitted.

```
  0x7fbfd6b19a90:   jne    0x7fbfd6b19acb  ;*monitorenter
  0x7fbfd6b19a96:   mov    0x14(%r11),%eax
  0x7fbfd6b19a9a:   add    %eax,%r9d
  0x7fbfd6b19a9d:   add    %eax,%r9d
  0x7fbfd6b19aa0:   add    %eax,%r9d
  0x7fbfd6b19aa3:   add    %eax,%r9d
  0x7fbfd6b19aa6:   add    %eax,%r9d
  0x7fbfd6b19aa9:   add    %eax,%r9d
  0x7fbfd6b19aac:   add    %eax,%r9d
  0x7fbfd6b19aaf:   add    %eax,%r9d       ;*monitorexit
  0x7fbfd6b19ab2:   mov    %r9d,%eax       ;*ireturn
```

<<IMG: LockCoarseningBenchmark_withoutBiasedLocking.svg>>

#### Conclusions:

- Similar trends as before, GraalVM EE JIT does a better job than GraalVM CE JIT and OpenJDK HotSpot C2 JIT

The `nested_synchronized` from OpenJDK HotSpot is (again) much slower than GraalVM EE/CE. The reason is the same, it does not get compiled by C1 nor C2, but Template Interpreter is used to generating the assembly code snippets for each bytecode.

By contrast, GraalVM EE JIT merges all the nested locks in one synchronized block and performs all the additions inside that synchronized block. The main difference is that now since the biased locking is disabled, a compare-and-swap atomic instruction guard that section.

```
0x7f3926b18d90:   lock cmpxchg %rsi,(%r11)   ;coarsed locking section
0x7f3926b18d95:   jne    0x7f3926b18e6b      ;*monitorexit 
0x7f3926b18d9b:   add    %r8d,%r9d           ;*iadd 
0x7f3926b18d9e:   add    %r8d,%r9d           ;*iadd 
0x7f3926b18da1:   add    %r8d,%r9d           ;*iadd 
0x7f3926b18da4:   add    %r8d,%r9d           ;*iadd 
0x7f3926b18da7:   add    %r8d,%r9d           ;*iadd 
0x7f3926b18daa:   add    %r8d,%r9d           ;*iadd 
0x7f3926b18dad:   add    %r8d,%r9d           ;*iadd 
0x7f3926b18db0:   add    %r8d,%r9d           ;*iadd 
0x7f3926b18db3:   mov    %r9d,%eax           ;*ireturn
```

Overall, running non-contented code with biased locking disabled adds the overhead of the compare-and-swap atomic operation, in comparison to the biased locking scheme, nevertheless, this comes by design.

### LockElisionBenchmark

Test how the compiler can elide/remove several adjacent locking blocks on non-shared objects, thus reducing the locking overhead.
Synchronization on non-shared objects is pointless, and runtime does not have to do anything there.

Source code: <<link to GitHub benchmark>>

<<IMG: LockElisionBenchmark.svg>>

#### Conclusions:

Except for `nested_synchronized` and `recursive_method_calls` benchmarks all three compilers behave similarly.

The `nested_synchronized` from OpenJDK HotSpot does not get compiled by C1 nor C2, but Template Interpreter is used to generating the assembly code snippets for each bytecode. This is similar to the LockCoarseningBenchmark (e.g., _make_not_compilable_).

```
  public int nested_synchronized() {
  int result = defaultValue << 1;

      Object lock = new Object();

      synchronized (lock) {
        result += incrementValue;
        synchronized (lock) {
          result += incrementValue;
          ...
        }
      }

      return result;
  }
```

The `recursive_method_calls` is explained in details for every compiler, as per below:

```
  public int recursive_method_calls() {
    int result = defaultValue << 1;
    result = recursiveSum(result, DEPTH);
    return result;
  }

  public int recursiveSum(int aValue, int depth) {
    Object lock = new Object();
    synchronized (lock) {
      if (depth == 0) {
        return aValue;
      }
      return incrementValue + recursiveSum(aValue, depth - 1);
    }
  }
```

- GraalVM EE JIT inlines all the recursive calls so there is no virtual method call involved anymore
- GraalVM CE JIT can remove the recursive calls, nevertheless, there is still a virtual method call involved.

The `recursive_method_calls` benchmark has only one virtual call to the recursive method `recursiveSum`
```
  0x7fed52ffc750:   mov    0x10(%rsi),%edx          ;*getfield defaultValue 
  0x7fed52ffc753:   mov    0x14(%rsi),%r10d         ;*getfield incrementValue 
  0x7fed52ffc757:   shl    %edx                     ;*ishl 
  0x7fed52ffc759:   mov    $0x2,%ecx 
  0x7fed52ffc75e:   mov    %r10d,0x4(%rsp)
  0x7fed52ffc763:   call   0x7fed52ffc240           ;*invokevirtual recursiveSum
  ...
  0x7fed52ffc793:   ret
```

and the `recursiveSum` (i.e., the recursive callee method) has no any virtual method call involved, everything get's inlined:

```
  0x7fed52ffc2b5:   add    0x4(%rsp),%eax               ;*iadd 
  0x7fed52ffc2b9:   add    0x4(%rsp),%eax               ;*iadd 
  0x7fed52ffc2bd:   add    0x4(%rsp),%eax               ;*iadd 
  0x7fed52ffc2c1:   mov    0x4(%rsp),%r10d
  0x7fed52ffc2c6:   add    %r10d,%eax                   ;*iadd 
  ...
  0x7fed52ffc2f0:   ret                                 ;*ireturn 
```

- OpenJDK HotSpot C2 JIT is not able to get rid of the recursive method calls, this is why it is the slowest

For example, the `recursiveSum` (i.e., the recursive method called from `recursive_method_calls` benchmark) has a call to itself:

```
  0x7f18dcf646d0:   mov    0x14(%rsi),%ebp      ;*getfield incrementValue 
  0x7f18dcf646d3:   cmp    $0x1,%ecx
  0x7f18dcf646d6:   je     0x7f18dcf646fb       ;*ifne 
  0x7f18dcf646d8:   add    $0xfffffffe,%ecx
  0x7f18dcf646db:   call   0x7f18dcf646c0       ;*invokevirtual recursiveSum 
  0x7f18dcf646e0:   add    %ebp,%eax            ;*invokevirtual recursiveSum 
  ...
  0x7f18dcf646f6:   ret
```

### LoopFusionBenchmark

The benchmark assesses if the compiler triggers loop fusion, an optimization aimed to merge the adjacent loops into one loop to reduce the loop overhead and improve run-time performance.

Source code: <<link to GitHub benchmark>>

<<IMG: LoopFusionBenchmark.svg>>

#### Conclusions:

None of the compilers has implemented this optimization.

Even though, an interesting case is why OpenJDK HotSpot VM runs faster than GraalVM EE/CE the `initial_loops` benchmark.
```
  public void initial_loops() {
    for (int i = 1; i < size; i++) {
      A[i] = A[i - 1] + B[i];
    }

    for (int i = 1; i < size; i++) {
      B[i] = B[i - 1] + A[i];
    }
  }
```

The OpenJDK HotSpot C2 JIT unrolls each of these loops by a factor of 8. For example, the code from below pertains to the first loop:

For example, the code snapshot from below pertains to the first loop:
```
  0x7f880cf64940:   mov    0x10(%rsi,%r10,4),%edx       ;*iaload
  0x7f880cf64945:   add    0xc(%rax,%r10,4),%edx        ;*iadd
  0x7f880cf6494a:   mov    %edx,0x10(%rax,%r10,4)       ;*iastore
  0x7f880cf6494f:   add    0xc(%rsi,%r10,4),%edx        ;*iadd
  0x7f880cf64954:   mov    %edx,0x10(%rsi,%r10,4)       ;*iastore
  ...
  0x7f880cf64a08:   add    $0x8,%r10d                   ;*iinc (loop is unroled by a factor of 8)
  0x7f880cf64a0c:   cmp    %ecx,%r10d
  0x7f880cf64a0f:   jl     0x7f880cf64940               ;*goto
```

GraalVM EE JIT emits the same instructions, nevertheless, the loop unrolling factor is 4 (as opposed to 8).

GraalVM CE JIT emits less optimal code instruction (i.e., more stores), even though the unrolling factor is 8 (similar to OpenJDK HotSpot C2 JIT).

```
  0x7f1d9affc620:   mov    0x10(%r9,%r11,4),%r8d      ;*iaload
  0x7f1d9affc628:   add    0xc(%rcx,%rdi,4),%r8d      ;*iadd
  0x7f1d9affc62d:   mov    %r8d,0x10(%rcx,%r11,4)     ;*iastore
  0x7f1d9affc632:   mov    0x14(%r9,%rdi,4),%r8d      ;*iaload
  0x7f1d9affc637:   add    0x10(%rcx,%r11,4),%r8d     ;*iadd
  0x7f1d9affc63c:   mov    %r8d,0x14(%rcx,%rdi,4)     ;*iastore
  ...
  0x7f1d9affc69b:   lea    0x8(%r11),%r11d            ;*iinc (loop is unroled by a factor of 8)
  0x7f1d9affc6a0:   cmp    %r11d,%ebx
  0x7f1d9affc6a3:   jg     0x7f1d9affc620
```

### LoopInvariantCodeMotionBenchmark

Test how the compiler deals with loop invariant code motion, in essence how it is able to move the invariant code before and after a loop.
Hoisting and sinking are terms that Compiler refers to moving operations outside loops:
- hoisting a load means to move the load so that it occurs before the loop
- sinking a store means to move a store to occur after a loop

Source code: <<link to GitHub benchmark>>

<<IMG: LoopInvariantCodeMotionBenchmark.svg>>

#### Conclusions:

GraalVM EE JIT looks better overall, nevertheless, a case where there is a noticeable difference is for the `initial_loop` benchmark.

The hot methods reported by the `-prof perfasm` belongs to the runtime-generated stub, so it makes it more difficult to grasp the optimizations from the benchmark method. Nevertheless, at first glance, based on the reported hottest methods, it looks like OpenJDK HotSpot JIT C2 spends the majority of time in the `StubRoutines::libmTan`, much more than GraalVM CE/EE JIT, which is probably an indication of fewer optimizations.

OpenJDK HotSpot VM hottest methods report:

```
  ....[Hottest Methods (after inlining)]..............................
  71.71%        runtime stub  StubRoutines::libmTan
  12.97%          libjava.so  jatan
  6.09%         c2, level 4  LoopInvariantCodeMotionBenchmark::initial_loop, version 4, compile id 479
  4.79%    Unknown, level 0  java.lang.StrictMath::atan, version 1, compile id 474
  2.61%          libjava.so  jfabs
```

GraalVM CE hottest methods report:
```
  ....[Hottest Methods (after inlining)]..............................
  45.01%                       <unknown>
  27.19%           libjava.so  jatan
  10.66%       jvmci, level 4  LoopInvariantCodeMotionBenchmark::initial_loop, version 3, compile id 729
  8.53%     Unknown, level 0  java.lang.StrictMath::atan, version 1, compile id 725
  5.57%           libjava.so  jfabs
```

GraalVM EE hottest methods report:
```
  ....[Hottest Methods (after inlining)]..............................
  37.50%                       <unknown>
  31.58%           libjava.so  jatan
  11.73%     Unknown, level 0  java.lang.StrictMath::atan, version 1, compile id 727
  9.98%       jvmci, level 4  LoopInvariantCodeMotionBenchmark::initial_loop, version 4, compile id 732
  6.57%           libjava.so  jfabs
```

A small remark, in the case of GraalVM CE/EE it would be better to have, instead of the `<unknown>` block, the exact dynamic shared object.  

### LoopReductionBenchmark

Loop reduction (or loop reduce) benchmark tests if a loop could be reduced by the number of additions within that loop.
This optimization is based on the induction variable to strength the additions.

Source code: <<link to GitHub benchmark>>

<<IMG: LoopReductionBenchmark.svg>>

#### Conclusions:

GraalVM EE JIT implements the reduction using a conditional move instruction (e.g., `cmovl`), as below: 

```
  0x7f40c2b1a3a5:   test   %edx,%edx
  0x7f40c2b1a3a7:   mov    $0x0,%eax
  0x7f40c2b1a3ac:   cmovl  %eax,%edx
  0x7f40c2b1a3af:   add    %edx,%ecx
  0x7f40c2b1a3b1:   mov    %ecx,%eax
```

OpenJDK HotSpot VM C2 JIT uses the same optimization but conditional jump instructions  are emitted (e.g., `cmp` and `jmp`).

The generated code by GraalVM EE JIT is abysmal:

```
                                                   ;loop 1
  0x7f4d56ffaf3b:   mov    $0x1,%r10d
  0x7f4d56ffaf41:   jmp    0x7f4d56ffaf65          ;*iload_3
  0x7f4d56ffaf60:   inc    %ecx                    ;*iinc
  0x7f4d56ffaf62:   inc    %r10d                   ;*iinc
  0x7f4d56ffaf65:   cmp    %r10d,%eax
  0x7f4d56ffaf68:   jg     0x7f4d56ffaf60
          
  0x7f4d56ffaf6a:   lea    -0x10(%rdx),%eax
  0x7f4d56ffaf6d:   mov    %r10d,%r11d
  0x7f4d56ffaf70:   mov    %ecx,%r10d
                                                   ;loop 2
  0x7f4d56ffaf73:   jmp    0x7f4d56ffaf88
  0x7f4d56ffaf80:   lea    0x10(%r11),%r11d        ;*iinc
  0x7f4d56ffaf84:   lea    0x10(%r10),%r10d        ;*iinc
  0x7f4d56ffaf88:   cmp    %r11d,%eax
  0x7f4d56ffaf8b:   jg     0x7f4d56ffaf80
  0x7f4d56ffaf8d:   jmp    0x7f4d56ffafb0
                                                   ;loop 3
  0x7f4d56ffafa0:   inc    %r11d                   ;*iinc
  0x7f4d56ffafa3:   inc    %r10d                   ;*iinc
  0x7f4d56ffafb0:   cmp    %r11d,%edx
  0x7f4d56ffafb3:   jg     0x7f4d56ffafa0          ;*if_icmpge
  0x7f4d56ffafb5:   mov    %r10d,%eax              ;*ireturn
  0x7f4d56ffafc1:   ret
```

### NpeControlFlowBenchmark

Iterates through an array of objects (containing either null and not null values) and compute the sum of all elements using different comparison/filtering strategies.
Since the array elements might be null, some tests explicitly check for null others just uses the objects as is but guard the code by try {} catch blocks (letting NullPointerException be thrown and catch).

Source code: <<link to GitHub benchmark>>

<<IMG: NpeControlFlowBenchmark.svg>>

#### Conclusions:

OpenJDK HotSpot C2 JIT behaves better than GraalVM CE/EE JIT 

In particalar, the `try_npe_catch` benchmark is orders of magnitude slower with GraalVM CE/EE JIT:
```
  public int try_npe_catch() {
    int sum = 0;
    for (int i = 0; i < size; i++) {
      try {
        sum += array[i].x;
      } catch (NullPointerException ignored) {
        sink(ignored);
      }
    }
    return sum;
  }
```

GraalVM EE JIT unrolls the main loop by a factor of 8 but there is no explicit null check while accessing array elements: 

```
                                                        ;<--- Loop Begin
  0x7f6a6ab1ac80:   mov    0x10(%rax,%r9,4),%ecx        ;*aaload 
  0x7f6a6ab1ac85:   mov    %r8d,%ebx                    ;*iload_2
  0x7f6a6ab1ac88:   add    0xc(,%rcx,8),%ebx            ;implicit exception: dispatches to 0x7f6a6ab1adab
  0x7f6a6ab1ac8f:   mov    0x14(%rax,%r9,4),%ecx        ;*iload_2
  0x7f6a6ab1ac94:   mov    0xc(,%rcx,8),%ecx            ;implicit exception: dispatches to 0x7f6a6ab1adc7
  0x7f6a6ab1ac9b:   mov    0x18(%rax,%r9,4),%edi        ;*iload_2
  0x7f6a6ab1aca0:   mov    0xc(,%rdi,8),%edi            ;implicit exception: dispatches to 0x7f6a6ab1ade6
  ...  
  0x7f6a6ab1ace8:   add    %ecx,%ebx
  0x7f6a6ab1acea:   add    %edi,%ebx
  ...  
  0x7f6a6ab1acfd:   lea    0x8(%r9),%r9d                ;*iinc
  0x7f6a6ab1ad01:   mov    %ebx,%r8d                    ;*iload_2
  0x7f6a6ab1ad04:   cmp    %r9d,%r11d
  0x7f6a6ab1ad07:   jg     0x7f6a6ab1ac80               ;<--- Loop Begin
```

Instead of checking for null, it just lets SIGSEGV happen and (in case there are nulls) dispatches the execution to a specific handler. If there are a lot of null values the cost of this mechanism is not neglectable.

GraalVM CE JIT does not do any loop unrolling but relies on the same optimistic approach (nulls are likely to happen) that takes a toll.

By contrast, OpenJDK HotSpot C2 JIT the null checks in the final optimized code version (within the unrolled loop instructions).

```
  0x7fae4cf666d0:   mov    0x14(%rdi,%r11,4),%r10d      ;*aaload
  0x7fae4cf666d5:   test   %r10d,%r10d                  ;null check
  0x7fae4cf666d8:   je     0x7fae4cf66751               ;*getfield x
  0x7fae4cf666de:   add    0xc(%r12,%r10,8),%ebx        ;*iadd
  ...
  0x7fae4cf666e3:   mov    0x18(%rdi,%r11,4),%edx       ;*aaload
  0x7fae4cf666e8:   test   %edx,%edx                    ;null check 
  0x7fae4cf666ea:   je     0x7fae4cf66756               ;*getfield x
  0x7fae4cf666f0:   add    0xc(%r12,%rdx,8),%ebx        ;*iadd
```

On modern hardware testing for null is probably cheap and it could bring a significant improvement for datasets where nulls are dominating (versus handling them in the SIGSEGV routines).

## Garbage Collectors

### Benchmarks Description

The Garbage Collectors included in the benchmarks are:
- Serial GC
- Parallel GC
- Garbage First (G1) GC (default)
- Shenandoah GC
- ZGC
- Epsilon GC (experimental, included just to have a baseline in a few cases)

The current GC benchmarks focus on below metrics:
- the efficiency of a GC objects allocation/reclamation with
  - different allocation rates
  - different object sizes
  - different pre-allocated objects on the heap
  - different number of allocator threads (e.g., 1 or 2)
- the impact of the barriers (e.g., read, write barriers) while traversing and/or updating heap data structures, trying to avoid any explicit allocation, in the benchmark method, unless it is induced by the underlying ecosystem.

### GCs Overview

#### Serial GC
- generational collector, stop-the-world (STW) pauses
- it has the smallest footprint and is desired especially for resource-constrained environments where latency is not an issue

#### Parallel GC
- generational collector, stop-the-world (STW) pauses
- also called a throughput collector, it is desired for applications where throughput is more important than latency

#### G1 GC
- generational collector, region based, stop-the-world (STW) pauses, concurrent phases
- strives for a balance between latency and throughput (with a desired maximum pause time of 200 ms)

#### Shenandoah GC
- uni-generational, region based (like G1 GC), fully concurrent
- target low-latency applications (for both large-heaps but also resource-constrained environments) with a few ms target pause times

#### ZGC
- uni-generational, mostly concurrent phases, but there are some STW pauses
- target low-latency applications (for both large-heaps but also resource-constrained environments) with a few ms target pause times (similar to Shenandoah GC)

>Note: In general, running single-threaded workloads against concurrent GCs could result in better throughput (or lower pause times) than in the case of the STW collectors, because the concurrent collectors are able to offload the GC work on otherwise idle cores.

### GC Barriers

Most GCs require different barriers that need to be implemented in the runtime, interpreter, C1 and C2. Such barriers affect application performance even when no actual GC work is happening. Below is a summary of such barriers mostly specific to each GC from JDK 17.

#### Epsilon GC
- does not add any barrier on top of the default/shared barriers (e.g., C1 or C2 compiler barriers). It might be the baseline (since it has the smallest overhead) in comparison to all the others, nevertheless it is still experimental at the moment.

#### Serial GC
- a card-table write barrier (to track the references from Tenured Generation to Young Generation). In this technique, the heap is partitioned into equal-sized cards, and a card table array is allocated, with an entry for each card of the heap. Card table entries are initially clean; the mutator write barrier marks the card containing the updated field (or the head of the object containing the field, in a variant) dirty. The collector must scan the card table to find dirty entries, and then scan the corresponding dirty cards to find the cross-generational pointers, if any, created by the writes.

#### Parallel GC
- a card-table write barrier (similar to Serial GC)

#### G1 GC
- a pre-write barrier is required in case of concurrent marking by Snapshot-At-The-Beginning (SATB) to make sure all objects live at the start of the marking are kept alive, all the reference updates need to any previous reference stored before writing
- a post-write barrier to track object references between different regions to enable the evacuation of old regions, which is done as part of mixed collections. References are tracked in remembered sets and are continuously updated with the help of the post-barrier
- a read barrier is added when a read access is performed to the referent field of a java.lang.ref.Reference. This will result in the referent being marked live
- two arraycopy barriers (e.g., pre-write and post-write barriers)
- CAS object barrier that handles atomic compare and swap related methods
- an object clone barrier to deal with cloning objects on the heap

#### Shenandoah GC
- load-reference barrier employed after a load from a reference field or array element and before the loaded object is given to the rest of the application code
- Snapshot-At-The-Beginning (SATB) write barrier employed before reference writes and used in case of concurrent marking. This is very similar to G1's SATB barrier, it intercepts writes and marks through "previous" objects
- an arraycopy barrier to deal with array copies (a better arraycopy-barrier-scheme in comparison to, for example, the G1's arraycopy barriers implemented in the C1/C2 or GraalVM JIT)
- CAS object barrier that handles atomic compare and swap related methods
- an object clone barrier to deal with cloning objects on the heap
- a keep-alive barrier for java.lang.ref.Reference to handle the cases when the referent field of a java.lang.ref.Reference object is read
- nmethod-barriers (i.e., a mechanism to arm nmethods) for concurrent unloading. For example, code cache unloading needs to know about on-stack nmethods. Arming the nmethods enables GC callbacks when they are called.
- stack-watermark barrier for concurrent thread-scanning

> Note: depending on the shenandoah mode, some of these barriers might be disabled!

#### ZGC
- load-reference barrier employed when references are loaded from the Heap. It ensures that
  references pointing into a relocated memory area will be caught and handled (i.e., the load barrier performs further actions to remap pointers that actually point into the relocated memory area, without the need to read that memory)
  references pointing into the newly allocated memory will pass through the barrier without further actions
- weak load barrier may be provided for loading a pointer “weakly” (i.e., load the pointer without keeping its target object alive in the context of garbage collection). Examples where a weak load barrier may be useful include reading a StringTable object in the JVM
- arraycopy barriers to deal with array copies
- CAS object barrier that handles atomic compare and swap related methods
- an object clone barrier to deal with cloning objects on the heap
- a keep-alive barrier used during the concurrent marking phase
- a mark barrier used during the concurrent marking phase
- nmethod-barriers and stack-watermark barriers (similar to Shenandoah GC)

> Note: At the moment GraalVM CE/EE (i.e., HotSpot based mode) does not support neither ZGC nor Shenandoah GC. For example, the Graal compiler does not implement any specific Shenandoah/ZGC barrier. This is the primary reason why it makes no sense to compare these GCs across different JVMs (OpenJDK HotSpot vs. GraalVM CE/EE).

### BurstHeapMemoryAllocatorBenchmark

This benchmark allocates arrays of temporary objects until it fills up a certain percent of the heap (e.g., 30%, 60%) and then releases them so that they all become eligible for Garbage Collector.

Source code: <<link to GitHub benchmark>>

<<IMG: BurstHeapMemoryAllocatorBenchmark_1thread_openjdk_hotspot_vm.svg>>

#### Conclusions:
- ZGC performs significantly better than all the other collectors (around 3x times faster than G1 GC), followed by G1 GC, ShenandoahGC, and Serial GC (for both 30% but also 60% allocated heap)
- Parallel GC offers the worst throughput

<<IMG: BurstHeapMemoryAllocatorBenchmark_2threads_openjdk_hotspot_vm.svg>>

#### Conclusions:
- Increasing the number of allocator threads minimizes the gap between the ZGC and G1 GC, the latter one performing even better in case of the 60% allocated heap
- Parallel GC offers (again, for this benchmark) the worst throughput, about ~10x slower than the best throughput

In general, under heavy burst allocations (e.g., 60% of the heap), the generational collectors (e.g., Serial GC, Parallel GC) fall into premature full GCs syndrome causing longer STW pauses in comparison to concurrent, single-generational, collectors.

### HeapMemoryAllocatorWithConstantRetrainedHeapBenchmark

This benchmark initially allocates (during setup) lists of chained objects (e.g., Object 1 -> Object 2 -> … ), until it fills up a certain percent of the heap (e.g., 25%, 50%, 75%). Each object list (i.e., the list header) is stored in an array-based structure that keeps strong references to each chain.
Such a chain looks like (head) Object 1 -> Object 2 -> … -> Object 32 where every object consists of a pointer to the next object and,in addition, an array of allocated longs. Some objects within the chain are potentially considered big, so they would normally follow the slow path allocation, residing directly in the Tenured Generation (in the case of generational collectors), increasing the likelihood of full GCs.
The chaining might have an impact on the GC roots traversal, since the degree of pointer indirection (i.e., reference processing) is not negligible, while traversing the object graph dependencies.
Then, in the benchmark method, similar object chains are allocated, and they replace, one by one (i.e., incrementally),the ones from the initial array so that the former ones become eligible for garbage collection.
During the lifecycle of the benchmark, the footprint of live memory is (trying to be) kept constant.

Source code: <<link to GitHub benchmark>>

<<IMG: HeapMemoryAllocatorWithConstantRetrainedHeapBenchmark_1thread_openjdk_hotspot_vm.svg>>

<<IMG: HeapMemoryAllocatorWithConstantRetrainedHeapBenchmark_2thread2_openjdk_hotspot_vm.svg>>

#### Conclusions:
- ZGC offers the highest throughput when the percentage of the heap is up to 50%
- when the retained heap (i.e., ballast) is 70%, G1 GC followed by the Parallel GC offers a better throughput
- Serial GC has the lowers throughput in all the cases

The cost of marking the entire heap to reclaim the garbage is not neglectable for the non-generational, concurrent, collectors (ZGC and Shenandoah GC) in cases where the ballast takes a significant part of the heap (e.g., 70% of the heap), otherwise easy to remove (young) garbage by the generational GCs (e.g., G1 GC, Parallel GC). This might be partially mitigated by increasing the number of concurrent GC threads using -XX:ConcGCThreads=<n>.

### HeapMemoryAllocatorWithFixedRetrainedHeapBenchmark

This benchmark initially allocates (during setup) lists of chained objects (e.g., Object 1 -> Object 2 -> … ), until it fills up a certain percent of the heap (e.g., 30%, 60%). Each object list (i.e., the list header) is stored in an array-based structure that keeps strong references to each chain.
Such a chain looks like (head) Object 1 -> Object 2 -> … -> Object 32 where every object consists of a pointer to the next object and, in addition, an array of allocated longs. Some objects within the chain are potentially considered big, so they would normally follow the slow path allocation, residing directly in the Tenured Generation (in the case of generational collectors), increasing the likelihood of full GCs.

The chaining might have an impact on the GC roots traversal, since the degree of pointer indirection (i.e., reference processing)  is not negligible, while traversing the object graph dependencies.
Then, in the benchmark method, similar object chains are allocated until they fill up 80% of the entire heap and then immediately released, so they become eligible for Garbage Collector.
During the lifecycle of the benchmark, the amount of retained memory by strong references is fixed (i.e., the objects allocated in the benchmark setup phase are kept alive during the benchmark method)

Source code: <<link to GitHub benchmark>>

<<IMG: HeapMemoryAllocatorWithFixedRetrainedHeapBenchmark_1thread_openjdk_hotspot_vm.svg>>

<<IMG: HeapMemoryAllocatorWithFixedRetrainedHeapBenchmark_2threads_openjdk_hotspot_vm.svg>>

#### Conclusions:
- G1 GC and/or Parallel GC offers (in general) the highest throughput in both benchmarks

The cost of marking the entire heap to reclaim the garbage strikes even worse (in this benchmark) the non-generational, concurrent, collectors (ZGC and Shenandoah GC).
One additional remark: since ZGC has Compressed OOPs disabled by design, within the same amount of memory, ZGC will allocate less number of objects (hence less objects to collect), in comparison to the other collectors that have Compressed OOPs enabled by default (in the case of Heap sizes up to 32 GB and on 64-bit platforms). Or, the other way around: trying to allocate the same number of objects (as the other collectors)  ZGC would need more memory for such allocations.

### HeapMemoryBandwidthAllocatorBenchmark

This benchmark tests the allocation rate for chunks of byte arrays having different sizes. In comparison to the previous benchmarks (e.g., HeapMemoryAllocatorWithConstantRetrainedHeapBenchmark, HeapMemoryAllocatorWithFixedRetrainedHeapBenchmark), it just allocates the byte arrays and immediately releases them, without keeping any strong references.

Source code: <<link to GitHub benchmark>>

<<IMG: HeapMemoryBandwidthAllocatorBenchmark_1thread_openjdk_hotspot_vm.svg>>

<<IMG: HeapMemoryBandwidthAllocatorBenchmark_2threads_openjdk_hotspot_vm.svg>>

#### Conclusions:
- for array chunks of 32 B and 8 MB Parallel GC and Serial GC seem to offer a tinny higher throughput, but not by a relevant difference
- for array chunks of 16 KB Serial GC and ZGC looks slightly better, but not by a relevant difference

###  ReadBarriersChainOfReferencesBenchmark

Test the overhead of read barriers while iterating through a chain of pre-allocated objects (e.g., Object 1 -> Object 2 -> ...) and returns the sum of their field properties (e.g., Object1.field + Object2.field + ...)

Source code: <<link to GitHub benchmark>>

<<IMG: ReadBarriersChainOfReferencesBenchmark_openjdk_hotspot_vm.svg>>

#### Conclusions:

- ZGC offers the best throughput, around 1.5x times better than the others
- Surprisingly, Epsilon GC since it does not have any barrier in the GC set, is not the leader position here

###  ReadBarriersLoopingOverArrayBenchmark

Test the overhead of read barriers while iterating through an array of pre-allocated objects and reading each object field.
Note: looping over an array favors algorithms that can hoist the barrier without accounting really on the cost of the barrier itself.

Source code: <<link to GitHub benchmark>>

<<IMG: ReadBarriersLoopingOverArrayBenchmark_openjdk_hotspot_vm.svg>>

#### Conclusions:
- ZGC offers the lowest throughput (~ 1.5x times slower than the highest throughput). It could be explained by the absence of Compressed OOPs. The array the benchmark walks, and the accessed objects are larger and have less chances to be cached in same-sized CPU caches (i.e., more CPU cache misses) in comparison to the other collectors using Compressed OOPs.

### ReadWriteBarriersBenchmark

Test the overhead of read/write barriers while iterating through an array of Integers and exchanging the values between two array entries (i.e., array[i] <-> array[j]).

Source code: <<link to GitHub benchmark>>

<<IMG: ReadWriteBarriersBenchmark_openjdk_hotspot_vm.svg>>

#### Conclusions:
- Epsilon GC, since it does not use any GC barrier, offers the best throughput. It was included to have a baseline in comparison to other collectors
- ZGC seems to perform better than any other collector
- G1 GC offers the worst throughput, more than  ~10x-14x slower than the rest. In such a case, the post-write barriers (i.e., remembered sets management across G1 regions) might be the reason behind.

###  WriteBarriersLoopingOverArrayBenchmark

Test the overhead of write barriers while iterating through the elements of an array of objects and updating every element (i.e., reference).

Source code: <<link to GitHub benchmark>>

<<IMG: WriteBarriersLoopingOverArrayBenchmark_openjdk_hotspot_vm.svg>>

#### Conclusions:
- Epsilon GC, since it does not use any GC barrier, offers the best throughput. It was included to have a baseline in comparison to other collectors
- Shenandoah GC seems to perform better than any other collector, but with a marginal difference in comparison to ZGC
- G1 GC offers the worst throughput, about ~10x-20x slower than the rest. Most probably it has the same root cause as in the previous benchmark (i.e. post-write barriers overhead)

### Geometric Mean

The Geometric Mean (GM) for the Garbage Collector benchmarks on each architecture is described below.

### OpenJDK HotSpot VM on x86_64 

No. | JVM distribution | Arcitecture | Geometric Mean | Unit
----|------------------|-------------|----------------|--------
1   | ZGC              | x86_64      | 43,267.97      | ops/ms
2   | G1 GC            | x86_64      | 29,158.73      | ops/ms
3   | Parallel GC      | x86_64      | 24,303.27      | ops/ms
4   | Shenandoah GC    | x86_64      | 23,475.71      | ops/ms
5   | Serial GC        | x86_64      | 17,129.77      | ops/ms

**Note:** The first in the row is the fastest, and the last in the row is the slowest GC

### OpenJDK HotSpot VM on arm64

No. | JVM distribution | Arcitecture | Geometric Mean | Unit
----|------------------|-------------|----------------|--------
1   | ZGC              | arm64       | 1,22,010.74    | ops/ms
2   | Shenandoah GC    | arm64       | 79,957.41      | ops/ms
3   | G1 GC            | arm64       | 79,251.23      | ops/ms
4   | Parallel GC      | arm64       | 62,358.15      | ops/ms
5   | Serial GC        | arm64       | 43,756.39      | ops/ms

**Note:** The first in the row is the fastest, and the last in the row is the slowest GC

To summarize, on both architectures:

1. ZGC offers the highest throughput
2. Shenandoah GC has a fluctuant position: it is better on arm64 and slower on x86_64

## Macro

This set of benchmarks (that we call it macro) is dedicated to larger programs using high-level Java APIs (e.g., stream, lambda, fork-join, etc.). It is created to complement the existing JIT and GC benchmarks with another class of benchmarks.

Note: we did similar tests on arm64 architecture, but for this we only show the geometrical mean.

### HuffmanCodingBenchmark

Huffman encoding is an algorithm devised by David A. Huffman of MIT in 1952 for compressing text data to make a file occupy a smaller number of bytes. This relatively simple compression algorithm is powerful enough that variations of it are still used today in computer networks, fax machines, modems, HDTV, and other areas.

The steps involved in Huffman encoding a given text source file into a destination compressed file are:
- count frequencies: examine a source file's contents and count the number of occurrences of each character
- build encoding tree: build a binary tree with a particular structure, where each node represents a character and its count of occurrences in the file. A priority queue is used to help build the tree along the way.
- build encoding map: traverse the binary tree to discover the binary encodings of each character
- encode data: re-examine the source file's contents, and for each character, output the encoded binary version of that character to the destination file.

Source code: <<link to GitHub benchmark>>

<<IMG: HuffmanCodingBenchmark.svg>>

### PalindromeBenchmark

Iterates through a list of Strings read from a file and checks, for each String, if it is a palindrome.
The benchmark uses a few alternative approaches:
- trampolines
- recursive
- iterative

The trampoline pattern is used for implementing algorithms recursively but without blowing the stack (as an alternative to recursive functions). A trampoline is an iteration applying a list of functions, where each function returns the next function to be called.

Source code: <<link to GitHub benchmark>>

<<IMG: PalindromeBenchmark.svg>>

### PopulationVarianceBenchmark

This benchmark generates a population of different ages and then calculates the age variation.
Population variance is the average of the distances from each data point in a particular population to the mean squared. It indicates how data points spread out in the population. Population variance is an important measure of dispersion used in statistics.

Source code: <<link to GitHub benchmark>>

<<IMG: PopulationVarianceBenchmark.svg>>

### PrimesBenchmark

Computes the number of prime numbers until a threshold (e.g., N) number. The benchmark uses a few alternative approaches:
- sieve of Eratosthenes
- a stream of prime numbers

Source code: <<link to GitHub benchmark>>

<<IMG: PrimesBenchmark.svg>>

### WordFrequencyBenchmark

Computes the word frequencies/occurrences from a text file. The benchmark uses a few alternative approaches:
- iterative
- parallel streams
- pattern streams

Source code: <<link to GitHub benchmark>>

<<IMG: WordFrequencyBenchmark.svg>>

### Geometric Mean

The Geometric Mean (GM) for the macro benchmarks category on each architecture is described below.

### x86_64

No. | JVM distribution   | Arcitecture | Geometric Mean | Unit
----|--------------------|-------------|----------------|--------
1   | GraalVM EE         | x86_64      | 4,224,822.25   | ns/op
2   | OpenJDK HotSpot VM | x86_64      | 4,638,065.96   | ns/op
3   | GraalVM CE         | x86_64      | 5,028,870.69   | ns/op

**Note:** The first in the row is the fastest, and the last in the row is the slowest GC

### arm64

No. | JVM distribution   | Arcitecture | Geometric Mean | Unit
----|--------------------|-------------|----------------|--------
1   | GraalVM EE         | arm64       | 2,425,617.28   | ns/op
2   | OpenJDK HotSpot VM | arm64       | 2,614,295.98   | ns/op
3   | GraalVM CE         | arm64       | 2,899,319.25   | ns/op

**Note:** The first in the row is the fastest, and the last in the row is the slowest GC

To summarize, on both architectures the geometric mean is consistent:

1. GraalVM EE JIT is the fastest
2. OpenJDK HotSpot VM C2 was in the middle
3. and GraalVM CE JIT is the slowest